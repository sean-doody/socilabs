[{"content":"Introduction This advanced tutorial will go over how to create custom Twitter datasets by collecting data from the official Twitter API. It assumes you have already installed Python and Anaconda and are comfortable coding in Python.\nThe coding examples contained in this codelab can be implemented in your favorite text editor and run as a standalone Python program, or integrated into an IDE or notebook environment like Jupyter Lab.\nPrerequisites  Twitter Developer Account Twitter API Keys Python coding knowledge Familiarity with coding loops, dictionary objects, and indexing Familiarity with functions  This tutorial will use the following packages and versions:\n Python 3.9 Tweepy 4.1.0 Pandas 1.3.4 Twitter API v1.1  Setting up the Environment First, let\u0026rsquo;s create a fresh conda environment to install our packages and dependencies. Open up your terminal and execute the following command:\n$ \u0026gt; conda create -n twitter-api python=3.9 Follow the prompt and type y to proceed. Now, activate the environment by executing the following script:\n$ \u0026gt; conda activate twitter-api Your bash shell or terminal should look something like this:\n(twitter-api) $ \u0026gt; The environment name, in paretheses, indicates that we have successfully activated our environment. Now we can begin installing our dependencies.\nInstall Packages First, ensure pip is updated:\n(twitter-api) $ \u0026gt; pip install --upgrade pip For this tutorial, we will need to install:\n Tweepy Pandas NumPy  We can do this easily in one line:\n(twitter-api) $ \u0026gt; pip install --upgrade tweepy pandas numpy Setting Up Tweepy With Your Credentials First, let\u0026rsquo;s make sure we import our libraries:\nimport tweepy import pandas as pd import numpy as np In order to access the Twitter API, you must provide the API key, the API secret, and the bearer token provided by Twitter at the time you set up your app in the Twitter Developer dashboard. You will also need to generate an access token from within the Twitter Developer dashboard after you have created your app. You will need both your access token and access token secret to access the API.\nIt is never a good idea to hardcode your credentials into your program. Instead, save them in a separate markup file, such as a JSON file, from which they can be read into your script without having to visibly hardcode them.\nYou can create a document called keys.json that contains the following contents:\n{ \u0026#34;api_key\u0026#34;: \u0026#34;\u0026lt;copy-and-paste-your-api-key-here\u0026gt;\u0026#34;, \u0026#34;api_secret\u0026#34;: \u0026#34;\u0026lt;copy-and-paste-your-api-secret-key-here\u0026gt;\u0026#34;, \u0026#34;bearer_token\u0026#34;: \u0026#34;\u0026lt;copy-and-paste-your-bearer-token-here\u0026gt;\u0026#34;, \u0026#34;access_token\u0026#34;: \u0026#34;\u0026lt;copy-and-pate-your-access-token-here\u0026gt;\u0026#34;, \u0026#34;access_secret\u0026#34;: \u0026#34;\u0026lt;copy-and-paste-your-access-secret-token-here\u0026gt;\u0026#34; } Then, we can load our credentials into our Python script using the following code:\nimport json # Set the path to your credentials JSON file: credentials = \u0026#34;\u0026lt;path_to_your_credential_file\u0026gt;.json\u0026#34; with open(credentials, \u0026#34;r\u0026#34;) as keys: api_tokens = json.load(keys) Now, instead of hardcoding the keys into our Python app, we can pull them from the JSON file we just loaded. JSON files are treated as dictionaries when loaded into Python, meaning we can index them by their keys to extract data.\nTo see a list of the keys contained in a dictionary, we can use the .keys() method this way: api_tokens.keys(). The keys will match exactly the keys from the JSON credentials file.\nLet\u0026rsquo;s grab our credentials from the dictionary using key indexing:\n# Grab the API keys: API_KEY = api_tokens[\u0026#34;api_key\u0026#34;] API_SECRET = api_tokens[\u0026#34;api_secret\u0026#34;] BEARER_TOKEN = api_tokens[\u0026#34;bearer_token\u0026#34;] ACCESS_TOKEN = api_tokens[\u0026#34;access_token\u0026#34;] ACCESS_SECRET = api_tokens[\u0026#34;access_secret\u0026#34;] Connect to the Twitter API We can now connect to the Twitter API using our credentials. We do this by authenticating our app via Tweepy with our keys:\n# We use Tweepy\u0026#39;s OAuthHandler method to authenticate our credentials: auth = tweepy.OAuthHandler(API_KEY, API_SECRET) # Then, we set our access tokens by calling the auth object directly: auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET) # Finally, we can initialize the Twitter API.  # NOTE: we will be using this `api` object to interact # with Twitter from here on out: api = tweepy.API(auth) We should now be connected to the Twitter API. To verify that our connection works and our credentials have been appropriately set up, let\u0026rsquo;s just pull some tweets from your home timeline and print their text:\ntest_tweets = api.home_timeline() # Looping through the tweets: for tweet in test_tweets: print(tweet.text + \u0026#34;\\n\u0026#34;) Your console should return the text from a sample of tweets pulled from your timeline. Note: these will be specific to your followers.\nNow that we\u0026rsquo;re all set up, let\u0026rsquo;s get started!\nSearching Twitter for Data There are lots of ways you can search for data on Twitter. You can: follow and pull data from specific accounts; collect tweets grouped by hashtags; find tweets by keyword search; set custom filters; and so on. What sorts of data you need, and the methods you apply to obtain those data, will vary by each research project\u0026rsquo;s use case.\nHere, we will focus on using Tweepy\u0026rsquo;s search_tweets method to use keyword queries, which can include hashtags, to search Twitter for tweets.\nThe search_tweets Method Hashtags and keywords are an excellent way to gather topically relevant Twitter data. In this example, we are going to be searcing for tweets related to COVID-19.\nFirst, let\u0026rsquo;s get a sense for how Tweepy handles Twitter searches and returns results.\nLet\u0026rsquo;s pull 10 tweets that match the query #covid19. We do this by calling the search_tweets() method on our API object, setting a search query, and setting a count:\ntweets = api.search_tweets( q=\u0026#34;#covid19\u0026#34;, count=10 ) In this code snipper, q is short for query, which is set equal to #covid19 in this example. count tells Tweepy how many tweets to fetch from the Twitter API. Here, we tell Tweepy to only grab 10 tweets.\nImportantly, what is returned is not raw tweet text, but a list of Twitter Status objects. We can see this by checking the type of the first tweet—type(tweet[0])—which will return tweepy.models.Status. We will need to take a closer look at this object in order to iterate through it and pull out the data of interest.\nNavigating JSON \u0026amp; Dictionaries Status objects are based on the JSON data containing tweet and user info for each object. They can be searched two ways: (a.) by calling methods on the status object where each method is a key in the JSON data dictionary; or (b.) grabbing the raw JSON itself and iterating over it directly.\nFor example, if we wanted to get the text of the first tweet, we could call the text method by writing tweets[0].text. This corresponds to the text key in the Twitter JSON data. We can do this for other important parameters like id, created_at (the date of the tweet), and so on: any parameter returned by Twitter.\nWe can see every key available by iterating over the JSON dictionary and printing the key (the JSON is accessible by calling the _json method as illustrated below):\nfor key in tweets[0]._json.keys(): print(key) You should see a list like this:\ncreated_at id id_str text truncated entities metadata source in_reply_to_status_id in_reply_to_status_id_str in_reply_to_user_id in_reply_to_user_id_str in_reply_to_screen_name user geo coordinates place contributors retweeted_status is_quote_status quoted_status_id quoted_status_id_str retweet_count favorite_count favorited retweeted lang Great! Now we have a list of all the available data returned by Twitter. However, some of these keys return more, nested JSON. For example, the user key returns a nested JSON dictionary containing information about the poster. We must interact with this nested data the same way we interact with the top-level tweet data.\nWe can get a list of all the keys and the type of data they return with some simple Python code:\nfor key in tweets[0]._json.keys(): # We can use a functional string to print the key and its type: print(f\u0026#34;{key}:: type {type(tweets[0]._json[key])}\u0026#34;) You should see this:\ncreated_at :: type \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; id :: type \u0026lt;class \u0026#39;int\u0026#39;\u0026gt; id_str :: type \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; text :: type \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; truncated :: type \u0026lt;class \u0026#39;bool\u0026#39;\u0026gt; entities :: type \u0026lt;class \u0026#39;dict\u0026#39;\u0026gt; metadata :: type \u0026lt;class \u0026#39;dict\u0026#39;\u0026gt; source :: type \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; in_reply_to_status_id :: type \u0026lt;class \u0026#39;NoneType\u0026#39;\u0026gt; in_reply_to_status_id_str :: type \u0026lt;class \u0026#39;NoneType\u0026#39;\u0026gt; in_reply_to_user_id :: type \u0026lt;class \u0026#39;NoneType\u0026#39;\u0026gt; in_reply_to_user_id_str :: type \u0026lt;class \u0026#39;NoneType\u0026#39;\u0026gt; in_reply_to_screen_name :: type \u0026lt;class \u0026#39;NoneType\u0026#39;\u0026gt; user :: type \u0026lt;class \u0026#39;dict\u0026#39;\u0026gt; geo :: type \u0026lt;class \u0026#39;NoneType\u0026#39;\u0026gt; coordinates :: type \u0026lt;class \u0026#39;NoneType\u0026#39;\u0026gt; place :: type \u0026lt;class \u0026#39;NoneType\u0026#39;\u0026gt; contributors :: type \u0026lt;class \u0026#39;NoneType\u0026#39;\u0026gt; retweeted_status :: type \u0026lt;class \u0026#39;dict\u0026#39;\u0026gt; is_quote_status :: type \u0026lt;class \u0026#39;bool\u0026#39;\u0026gt; quoted_status_id :: type \u0026lt;class \u0026#39;int\u0026#39;\u0026gt; quoted_status_id_str :: type \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; retweet_count :: type \u0026lt;class \u0026#39;int\u0026#39;\u0026gt; favorite_count :: type \u0026lt;class \u0026#39;int\u0026#39;\u0026gt; favorited :: type \u0026lt;class \u0026#39;bool\u0026#39;\u0026gt; retweeted :: type \u0026lt;class \u0026#39;bool\u0026#39;\u0026gt; lang :: type \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; Now, we can see the type of data each key is associated with. str just means stirng, or text; int just means an integer; bool just means a value of either True or False; and NoneType just means the data is empty or not applicable and can safely be ignored.\nImportantly, though, notice that some keys contain class dict: this means that they are associated with a dictionary, and contain further nested data. Let\u0026rsquo;s use user as an example.\nTo see all of the keys in the user data, you can use this code:\nfor key in tweets[0].user._json.keys(): print(key) This returns all user-level keys:\nid id_str name screen_name location description url entities protected followers_count friends_count listed_count created_at favourites_count utc_offset time_zone geo_enabled verified statuses_count lang contributors_enabled is_translator is_translation_enabled profile_background_color profile_background_image_url profile_background_image_url_https profile_background_tile profile_image_url profile_image_url_https profile_banner_url profile_link_color profile_sidebar_border_color profile_sidebar_fill_color profile_text_color profile_use_background_image has_extended_profile default_profile default_profile_image following follow_request_sent notifications translator_type withheld_in_countries Again, to see if there is anymore nested data, you can check the data type returned with each key:\nfor key in tweets[0].user._json.keys(): print(f\u0026#34;{key}:: type {type(tweets[0].user._json[key])}\u0026#34;) Which returns:\nid :: type \u0026lt;class \u0026#39;int\u0026#39;\u0026gt; id_str :: type \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; name :: type \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; screen_name :: type \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; location :: type \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; description :: type \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; url :: type \u0026lt;class \u0026#39;NoneType\u0026#39;\u0026gt; entities :: type \u0026lt;class \u0026#39;dict\u0026#39;\u0026gt; protected :: type \u0026lt;class \u0026#39;bool\u0026#39;\u0026gt; followers_count :: type \u0026lt;class \u0026#39;int\u0026#39;\u0026gt; friends_count :: type \u0026lt;class \u0026#39;int\u0026#39;\u0026gt; listed_count :: type \u0026lt;class \u0026#39;int\u0026#39;\u0026gt; created_at :: type \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; favourites_count :: type \u0026lt;class \u0026#39;int\u0026#39;\u0026gt; utc_offset :: type \u0026lt;class \u0026#39;NoneType\u0026#39;\u0026gt; time_zone :: type \u0026lt;class \u0026#39;NoneType\u0026#39;\u0026gt; geo_enabled :: type \u0026lt;class \u0026#39;bool\u0026#39;\u0026gt; verified :: type \u0026lt;class \u0026#39;bool\u0026#39;\u0026gt; statuses_count :: type \u0026lt;class \u0026#39;int\u0026#39;\u0026gt; lang :: type \u0026lt;class \u0026#39;NoneType\u0026#39;\u0026gt; contributors_enabled :: type \u0026lt;class \u0026#39;bool\u0026#39;\u0026gt; is_translator :: type \u0026lt;class \u0026#39;bool\u0026#39;\u0026gt; is_translation_enabled :: type \u0026lt;class \u0026#39;bool\u0026#39;\u0026gt; profile_background_color :: type \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; profile_background_image_url :: type \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; profile_background_image_url_https :: type \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; profile_background_tile :: type \u0026lt;class \u0026#39;bool\u0026#39;\u0026gt; profile_image_url :: type \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; profile_image_url_https :: type \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; profile_banner_url :: type \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; profile_link_color :: type \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; profile_sidebar_border_color :: type \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; profile_sidebar_fill_color :: type \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; profile_text_color :: type \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; profile_use_background_image :: type \u0026lt;class \u0026#39;bool\u0026#39;\u0026gt; has_extended_profile :: type \u0026lt;class \u0026#39;bool\u0026#39;\u0026gt; default_profile :: type \u0026lt;class \u0026#39;bool\u0026#39;\u0026gt; default_profile_image :: type \u0026lt;class \u0026#39;bool\u0026#39;\u0026gt; following :: type \u0026lt;class \u0026#39;bool\u0026#39;\u0026gt; follow_request_sent :: type \u0026lt;class \u0026#39;bool\u0026#39;\u0026gt; notifications :: type \u0026lt;class \u0026#39;bool\u0026#39;\u0026gt; translator_type :: type \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; withheld_in_countries :: type \u0026lt;class \u0026#39;list\u0026#39;\u0026gt; It looks like the only further nested data for user is entities. To learn more about entities objects, check out the official Twitter API documentation.\nFamiliarizing yourself with these keys and their associated data structure is essential for effectively navigating the Twitter API. Take some time to explore these keys and familiarize yourself with Python dictionary objects if this feels confusing.\nChoosing which data you need is specific to your use case and research project. However, some data is absolutely necessary to maintain the integrity of your dataset. Four absolutely essential data are:\n The tweet id. The tweet created_at UTC timestamp. The tweet text. The tweet author. The weet author_id.  Almost certainly you will want and need more data, including hashtags, user mentions, whether or not a tweet is a retweet, favorite counts, and so on. Familiarizing yourself with the Twitter API, as well as Twitter itself, is absolutely necessary to determining which data you need.\nPaginating Twitter Data In the simple example given above, we searched Twitter for 10 tweets relevant to the query #covid19. When Twitter returns tweets, it does so in batches of JSON. The max size a batch can be is 100, meaning Twitter will only return 100 tweets per batch. In order to return more batches of Tweets, you will need to paginate the Tweets using Tweepy\u0026rsquo;s Cursor() method. Luckily, implementing pagination is extremely easy with Tweepy. Do note that you must be attentive to rate limitations on your API calls, which you can read more about in the Twitter API documentation.\nWhen searching and paginating Tweets with Tweepy\u0026rsquo;s Cursor(), we use the following syntax:\ntweets = tweepy.Cursor(api.search_tweets, q=\u0026#34;#covid19\u0026#34;, count=100).items(500) Using the Cursor() method, we instructed Tweepy to fetch 500 (.items(500)) total tweets in batches of 100 (count=100). In other words, we have 5 pages of Tweets. However, the tweets are not returned directly. Instead, Tweepy creates a generator object that we have to iterate through to extract each tweet.\nTo see how this works, let\u0026rsquo;s grab the text of the first 10 tweets:\n# Because it\u0026#39;s a generator, we need to have a counter: n_tweets = 0 for tweet in tweets: if n_tweets \u0026lt; 10: print(tweet.text + \u0026#34;\\n\u0026#34;) n_tweets += 1 else: pass The text of the first 10 tweets contained in the generator will be returned. We can simplify our code a bit by iterating over the generator a bit differently, as shown below:\nfor tweet in tweepy.Cursor(api.search_tweet, q=\u0026#34;#covid19\u0026#34;, count=5).items(20): print(tweet.text) This simplified syntax prevents us from having to create a seperate variable containing the Tweepy generator. Instead, we just loop through the generator and discard it after we\u0026rsquo;re done. Easy!\nNow let\u0026rsquo;s combine these insights to build a full working example that includes aggregating the tweets into a Pandas dataframe and turning the tweets into a functional dataset.\nA Working Example We\u0026rsquo;re now going to bring these pieces together and add some more specificty (and slightly more coplexity) to our Twitter scrape.\nFirst, let\u0026rsquo;s prepare the parameters for the search_tweets() method in separate variables. This allows us to change the content of the variable without having to manually change the query in our api object any time we want to modify our search terms. We will also tell Tweepy how many tweets we want per page and set the page limit. Importantly, we will introduce the parameter tweet_mode and set it to extended in order to get the full text of a tweet. While Twitter now allows 280 character tweets, by default, the Twitter API truncates tweets to 140 characters. We must override this. Additionally, the text parameter we have been using will now change to full_text.\n# We can use logical search operators in our query text. # Let\u0026#39;s add a series of hashtags with OR, meaning a tweet can # contain any of the search terms: query = \u0026#34;#covid19 OR #covid OR #covid-19 OR #coronavirus # We will also add a new parameter that limits us to English # results only: lang = \u0026#34;en\u0026#34; # Ensure extended is set to true: tweet_mode = \u0026#34;extended\u0026#34; # Let\u0026#39;s limit ourselves to 100 tweets per page: count = 100 # Let\u0026#39;s grab only 1000 tweets: tweet_limit = 1000 Now, we can use these variables are parameters in our Tweepy API without needing to hardcode them directly.\nfor tweet in tweepy.Cursor(api.search_comments, q=query, lang=lang, tweet_mode=tweet_mode, count=count).items(tweet_limit): \u0026lt;...do something...\u0026gt; While we have been simply printing the text of tweets, it is time to actually begin extracing the data we want from each tweet. Tweet data, such as a tweet\u0026rsquo;s text and ID, are not nested and can be easily extracted. However, user-level data is nested in a sub-dictionary, and data like hashtags and user-mentions are nested in sub-dictionaries that themselves contain further sub-dictionaries. As mentioned earlier, familiarizing yourself with the JSON structure of the Tweet data and Python\u0026rsquo;s dictionary class is essential. For now, let\u0026rsquo;s see some working examples.\nLet\u0026rsquo;s map out the variables of interest At different levels:\n  Tweet level:\n id created_at full_text hashtags user_mentions in_reply_to_user_id in_reply_to_screen_name is_quote_status Whether or not the tweet is a retweet (and if so, that tweet\u0026rsquo;s identifiers). retweet_count favorite_count    User level:\n The user\u0026rsquo;s id The user\u0026rsquo;s screen_name The user\u0026rsquo;s name verified status    Let\u0026rsquo;s see how to access this data by refactoring our code and wrapping everything together into a nice function that takes arguments for the API parameters, iterates through the tweets, and returns a nicely formatted Pandas dataframe.\nRefactoring Into a Function If you are not familiar with Python functions, consider taking some time to learn more about them. You are also free to follow along. Otherwise, let\u0026rsquo;s get started!\nLet\u0026rsquo;s define a Tweet scraping function that takes all of our API parameters of interest as arguments:\ndef tweet_scraper(query=None, lang=\u0026#34;en\u0026#34;, tweet_mode=\u0026#34;extended\u0026#34;, count=100, tweet_limit=1000): \u0026#34;\u0026#34;\u0026#34; This function takes Tweepy search_tweets parameters as arguments and returns a Pandas dataframe containing tweet data. :param query: a keyword search phrase (string) :param lang: limit results by language (default: English) :param tweet_mode: choose whether to extend tweets to full 280 characters. :param count: the number of tweets to return per page (default: 100; max: 100) :param tweet_limit: the maximum number of tweets to return (default: 1000). \u0026#34;\u0026#34;\u0026#34; # First, let\u0026#39;s create a dictionary that will store our tweet data. We # are using a dictionary because we can easily generate a Pandas dataframe # from the dictionary keys. # # The dictionary will be formatted so that its keys are parameters associated with # each tweet and its values are lists to which we will append results for each tweet: data = { \u0026#34;user_id\u0026#34;: [], \u0026#34;screen_name\u0026#34;: [], \u0026#34;name\u0026#34;: [], \u0026#34;verified\u0026#34;: [], \u0026#34;id\u0026#34;: [], \u0026#34;created_at\u0026#34;: [], \u0026#34;full_text\u0026#34;: [], \u0026#34;retweet_count\u0026#34;: [], \u0026#34;favorite_count\u0026#34;: [], \u0026#34;hashtags\u0026#34;: [], \u0026#34;user_mentions\u0026#34;: [], \u0026#34;in_reply_to_user_id\u0026#34;: [], \u0026#34;in_reply_to_screen_name\u0026#34;: [], \u0026#34;is_quote_status\u0026#34;: [], \u0026#34;is_retweet\u0026#34;: [], # we will have to build this parameter ourselves; see below \u0026#34;retweet_og_id\u0026#34;: [], # the ID of the original retweeted tweet \u0026#34;retweet_og_author_id\u0026#34;: [], # the original author ID of a retweeted tweet \u0026#34;retweet_og_author_screen_name\u0026#34;: [], # the original author screen name of a retweeted tweet \u0026#34;retweet_og_author_name\u0026#34;: [], # the original author\u0026#39;s name of a retweeted tweet \u0026#34;retweet_og_date\u0026#34;: [], # the date of the original tweet \u0026#34;retweet_og_full_text\u0026#34;: [], # OG full text of the retweet \u0026#34;retweet_og_retweet_count\u0026#34;: [], # OG retweet count \u0026#34;retweet_og_favorite_count\u0026#34;: [] # OG favorite count } # Search the tweets as we\u0026#39;ve already done, but this time, plug in the paremeter values # from the function arguments: for tweet in tweepy.Cursor(api.search_tweets, q=query, tweet_mode=tweet_mode, count=count).items(tweet_limit): \u0026#34;\u0026#34;\u0026#34; We need to start with user level variables, meaning we are going to iterate through the user dictionary. We can do this easily! Then, we are going to append the data to the list in our data dictionary. Let\u0026#39;s see how it\u0026#39;s done: \u0026#34;\u0026#34;\u0026#34; # User ID: data[\u0026#34;user_id\u0026#34;].append(tweet.user.id) # Screen name: data[\u0026#34;screen_name\u0026#34;].append(tweet.user.screen_name) # Name: data[\u0026#34;name\u0026#34;].append(tweet.user.name) # verified status: data[\u0026#34;verified\u0026#34;].append(tweet.user.verified) \u0026#34;\u0026#34;\u0026#34; Great! Now let\u0026#39;s grab the tweet level data: \u0026#34;\u0026#34;\u0026#34; # Tweet ID: data[\u0026#34;id\u0026#34;].append(tweet.id) # Date: data[\u0026#34;created_at\u0026#34;].append(tweet.created_at) # Full text of tweet: data[\u0026#34;full_text\u0026#34;].append(tweet.full_text) # Get retweet count: data[\u0026#34;retweet_count\u0026#34;].append(tweet.retweet_count) # Get favorite count: data[\u0026#34;favorite_count\u0026#34;].append(tweet.favorite_count) # NOTE: to get hashtags \u0026amp; user mentions, we need to iterate through # the entities sub dictionary. Then, we need to iterate through # the hashtag sub dictionary. It sounds bad, but it\u0026#39;s not!  # We will save the hashtags to a list and append the list # to our data dictionary: hashtags = [] # Try to get hashtags; if there is an error, then there are no hashtags # and we can pass: try: for hashtag in tweet.entities[\u0026#34;hashtags\u0026#34;]: hashtags.append(hashtag[\u0026#34;text\u0026#34;]) except Exception: pass # Now append the hashtag list to our dataset! If there are no # hashtags, just set it equal to NaN: if len(hashtags) == 0: data[\u0026#34;hashtags\u0026#34;].append(np.nan) else: data[\u0026#34;hashtags\u0026#34;].append(hashtags) # We do the same thing for user mentions: mentions = [] try: for mention in tweet.entities[\u0026#34;user_mentions\u0026#34;]: mentions.append(mention[\u0026#34;screen_name\u0026#34;]) except Exception: pass if len(mentions) == 0: data[\u0026#34;user_mentions\u0026#34;].append(np.nan) else: data[\u0026#34;user_mentions\u0026#34;].append(mentions) # In reply to user id: data[\u0026#34;in_reply_to_user_id\u0026#34;].append(tweet.in_reply_to_user_id) # In reply to user screen name: data[\u0026#34;in_reply_to_screen_name\u0026#34;].append(tweet.in_reply_to_screen_name) # Check if quote status: data[\u0026#34;is_quote_status\u0026#34;].append(tweet.is_quote_status) # We need to check if a tweet is a retweet ourselves. We can do this by checking # if the retweeted_status key is present in the JSON: if \u0026#34;retweeted_status\u0026#34; in tweet._json.keys(): # Then it is a retweet: data[\u0026#34;is_retweet\u0026#34;].append(True) # Get OG tweet id: data[\u0026#34;retweet_og_id\u0026#34;].append(tweet.retweeted_status.id) # Get OG author ID: data[\u0026#34;retweet_og_author_id\u0026#34;].append(tweet.retweeted_status.user.id) # Get OG author screen name: data[\u0026#34;retweet_og_author_screen_name\u0026#34;].append(tweet.retweeted_status.user.screen_name) # Get OG author name: data[\u0026#34;retweet_og_author_name\u0026#34;].append(tweet.retweeted_status.user.name) # Get date of OG tweet: data[\u0026#34;retweet_og_date\u0026#34;].append(tweet.retweeted_status.created_at) # Get OG full text: data[\u0026#34;retweet_og_full_text\u0026#34;].append(tweet.retweeted_status.full_text) # Get OG retweet count: data[\u0026#34;retweet_og_retweet_count\u0026#34;].append(tweet.retweeted_status.retweet_count) # Get OG favorite count: data[\u0026#34;retweet_og_favorite_count\u0026#34;].append(tweet.retweeted_status.favorite_count) else: # Set is_retweet to false and all other values to np.nan: data[\u0026#34;is_retweet\u0026#34;].append(False) data[\u0026#34;retweet_og_id\u0026#34;].append(np.nan) data[\u0026#34;retweet_og_author_id\u0026#34;].append(np.nan) data[\u0026#34;retweet_og_author_screen_name\u0026#34;].append(np.nan) data[\u0026#34;retweet_og_author_name\u0026#34;].append(np.nan) data[\u0026#34;retweet_og_date\u0026#34;].append(np.nan) data[\u0026#34;retweet_og_full_text\u0026#34;].append(np.nan) data[\u0026#34;retweet_og_retweet_count\u0026#34;].append(np.nan) data[\u0026#34;retweet_og_favorite_count\u0026#34;].append(np.nan) # Whoo! That\u0026#39;s a lot of code. Now, let\u0026#39;s turn our data dictionary into a Pandas dataframe # and then return it: df = pd.DataFrame(data) # Now send it out: return df We made it! Let\u0026rsquo;s call the function and save our data. We can preset our function arguments as separate variables, which will allow us to override default function values if we like, and then pass them into the function itself:\n# Set the function parameters: query = \u0026#34;#covid19 OR #covid OR #covid-19 OR #coronavirus lang = \u0026#34;en\u0026#34; tweet_mode = \u0026#34;extended\u0026#34; count = 100 tweet_limit = 1000 # Call the function using our parameters: df = tweet_scraper(query=query, lang=lang, tweet_mode=tweet_mode, count=count, tweet_limit=tweet_limit) Awesome! We now have our Tweet data in a structured, tabular, dataframe. Let\u0026rsquo;s inspect our data by calling Pandas\u0026rsquo;s head() method. This method allows you to the see the top N samples in your dataframe. By default, this is set to 5, but we can override ths by passing an argument into the parentheses. Let\u0026rsquo;s look at the top 10 posts:\ndf.head(10) You should see a result like this:\n    user_id screen_name name verified id \u0026hellip;     0 45823110 jude5456 Jude 🇪🇺 💙🌹 False 1450854059968184336 \u0026hellip;   1 4904619394 Suzyistdaheim 𝕊𝕦𝕫𝕪™ 😷💉💉🏠🍀 False 1450854059963977731 \u0026hellip;   2 138468171 captainpt Peter Schultz False 1450854058856751106 \u0026hellip;   3 2253036106 CRCrangelc Clodovaldo Rangel False 1450854058407956480 \u0026hellip;   4 3082370998 BeckyRae12345 Becca False 1450854058009534464 \u0026hellip;   \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip;    I have truncated the columns and rows. In total, we have 23 columns of data. You can see this for yourself simply by printing the length of the .columns attribute of the Pandas dataframe:\nprint(f\u0026#34;There are {len(df.columns)}columns\u0026#34;) Which returns:\nThere are 23 columns You can also see all the columns in the dataframe by printing them:\nfor column in df.columns: print(columns) Which should produce:\nuser_id screen_name name verified id created_at full_text retweet_count favorite_count hashtags user_mentions in_reply_to_user_id in_reply_to_screen_name is_quote_status is_retweet retweet_og_id retweet_og_author_id retweet_og_author_screen_name retweet_og_author_name retweet_og_date retweet_og_full_text retweet_og_retweet_count retweet_og_favorite_count Excellent! Now, we can move on to saving our data.\nSaving the Data Pandas makes saving dataframes as local files extremely easy. I strongly advise that we save our dataframe in JSON format because our dataframe contains columns populated by lists. Specifically, the hashtags and user_mentions columns both contain lists of hashtags and mentioned users, respectively. JSON natively supports lists. If we save the dataframe as a CSV file, these lists will be converted to plaintext strings. We do not want this.\nThere is a workaround, however: we can turn the lists into strings using Python\u0026rsquo;s join() method, and then re-split the strings into lists again as needed. We illustrate how to do this below.\nSaving as JSON To save as JSON without changing the list variabels, simply call the to_json() method:\ndf.to_json(\u0026#34;twitter_data.json\u0026#34;) And we\u0026rsquo;re done! You can name your datafile whatever you like.\nWhen you\u0026rsquo;re ready to load your json file again, we call the read_json() method and save it to a variable:\ndf = pd.read_json(\u0026#34;twitter_data.json\u0026#34;) Voila! The data is back.\nSaving to a CSV If you insist on saving your data in CSV format, you need to deal with the lists in the hashtags and user_mentions columns. To do this, we are going to create a function that converts the list to a string, where each list item is separated by a comma (,). In the future, if we want to convert the columns back to a list, we can simply split on the comma.\nLet\u0026rsquo;s see how this works:\n# Let\u0026#39;s create a function that cleans up the lists: def list_cleaner(list_object): \u0026#34;\u0026#34;\u0026#34; This function takes one argument: list_object, which is list. \u0026#34;\u0026#34;\u0026#34; # Let\u0026#39;s try to join the list. Note that we nest the join in a Try/Except # pattern. This is because not all Tweets contain either hashtags or user mentions. # In this case, they simply have a NaN missing value. This will throw an error if  # not dealt with: try: output = \u0026#34;,\u0026#34;.join(list_object) except Exception: output = list_object return output Now we can use Pandas\u0026rsquo;s apply() method to map the function to the columns and fix the lists. We do this by calling apply() on the column of interest and passing our function, wihout parentheses, as an argument:\n# Fix hashtags list: df[\u0026#34;hashtags\u0026#34;] = df[\u0026#34;hashtags\u0026#34;].apply(list_cleaner) # Fix mentions list: df[\u0026#34;user_mentions\u0026#34;] = df[\u0026#34;user_mentions\u0026#34;].apply(list_cleaner) Awesome! Our lists are now strings and can safely be saved as CSV. This is a simple one-liner:\ndf.to_csv(\u0026#34;twitter_data.csv\u0026#34;) And if we wanted to load the CSV file when we\u0026rsquo;re ready:\ndf = pd.read_csv(\u0026#34;twitter_data.csv\u0026#34;) To re-split the data back into a list after re-loading the CSV, we can write on more simple helper function:\ndef split_into_list(text): # We need exception handling again for missing values: try: # Remember: we joined on a comma, so let\u0026#39;s split on a comma: output = text.split(\u0026#34;,\u0026#34;) except Exception: output = text return output And then apply it to our columns again:\n# Split hashtags back into a list: df[\u0026#34;hashtags\u0026#34;] = df[\u0026#34;hashtags\u0026#34;].apply(split_into_list) # Split mentions back into list: df[\u0026#34;user_mentions\u0026#34;] = df[\u0026#34;user_mentions\u0026#34;].apply(split_into_list) Fantastic! Our hashtags and user mentions are back into a list.\nConclusion This tutorial covered how to build custom Twitter datasets using the Tweepy package in Python and the Twitter API. We covered a lot of ground and dug in to some complex coding examples. This is just the tip of the iceberg: there is so much more to cover! I strongly recommend you check out the official Tweepy documentation to familiarize yourself the ins and outs of the package.\nAlso, make sure you spend time reading through the official Twitter API documentation to learn all the nuances of Twitter\u0026rsquo;s developer platform. Going directly to the source is usually the best course of action!\nFuture tutorial series will look at different ways to analyze Twitter data. For now, though, thank you for reading this tutorial. Feel free to reach out if you have any questions.\nHappy coding!\n","date":"2021-10-19T00:00:00Z","image":"https://sean-doody.github.io/socilabs/p/tweepy-twitter-tutorial/twitter_hu96b8e6f01a60d9e713e886147ce39f04_119441_120x120_fill_q75_box_smart1.jpg","permalink":"https://sean-doody.github.io/socilabs/p/tweepy-twitter-tutorial/","title":"Use Tweepy \u0026 Python to Scrape the Twitter API"},{"content":"Python is an extremely popular programming language that has quickly become the language of choice for programmers, data scientists, and computational social scientists alike due to its rich ecosystem of statistical and data management libraries, a vibrant open source community, and its clean and easy to read syntax.\nWhile Python can be installed by itself from the official website, for most social science researchers, it makes much more sense to install Python as part of the Anaconda distribution. Anaconda is data science platform that supports all platforms—Windows, macOS, and Linux—and makes installing and managing Python and its scientific computing libraries a breeze.\nWhat You\u0026rsquo;ll Learn  How to install Python as part of the Anaconda distribution. How to install packages with conda and pip. How to create and manage programming environments with conda.  What You\u0026rsquo;ll Need\n A computer running a recent version of Windows, macOS, or Linux.WWWW  Let\u0026rsquo;s get started!\n Note: All tutorial examples will be illustrated using a Windows PC.\n Download the Installer  Click here to go to the Windows Anaconda download page (Mac users click here). On the site, you will see the following download landing page:   \n Click the green Download button. Save the exe file to a folder of your choice. When the download completes, continue to the next section of this tutorial.  Installation  Navigate to where you downloaded Anaconda.   Note: On PC, this will be a .exe file. On macOS, this will be a .dmg file.\n  Double click the file to launch the installer. Click Next. Read and accept the licensing the agreement. Choose if you want to install Anaconda for only you (\u0026ldquo;Just Me\u0026rdquo;) or every account on your computer.  As a rule of thumb, it is probably best to choose \u0026ldquo;Just Me.\u0026quot;   Choose where you\u0026rsquo;d like to install Anaconda and click Next.   Note: It is generally safest and recommended to install Anaconda in the default directory.\n  \n Important: Make sure both Add Anaconda3 to my PATH environment variable and Register Anaconda3 as my default Python 3.x are selected!   \n Click the Install button (this can take several minutes). Anaconda will offer you the option to install optional software packages. Choose if you\u0026rsquo;d like to try these software. Do note that they are completely optional: Anaconda will function perfectly fine without them. When the installer completes, click Finish to exit the installer.   Note: We will be illustrating how to use Anaconda from your computer\u0026rsquo;s terminal (e.g., command line on Windows, Terminal on macOS and Linux). This is the recommended usage of Anaconda. If you would like to use the Anaconda GUI, consult the documentation on the Anaconda Navigator application.\n The following steps should work on all operating systems.\nCheck for Anaconda:  Launch your computer\u0026rsquo;s terminal app.  Windows: click Start and type cmd; right-click Command Prompt and click Run as Administrator. macOS: open your Terminal app; if Terminal is not in your dock, you can find it in the Launchpad.   In your computer\u0026rsquo;s terminal window, type conda just as below and hit enter.  $ \u0026gt; conda  Your terminal should output a message similar to the one below:  usage: conda-script.py [-h] [-V] command ... conda is a tool for managing and deploying applications, environments and packages. Options: positional arguments: command clean Remove unused packages and caches. compare Compare packages between conda environments. config Modify configuration values in .condarc. This is modeled after the git config command. Writes to the user .condarc file (C:\\Users\\[USERNAME]\\.condarc) by default. create Create a new conda environment from a list of specified packages. help Displays a list of available conda commands and their help strings. info Display information about current conda install. init Initialize conda for shell interaction. [Experimental] install Installs a list of packages into a specified conda environment. list List linked packages in a conda environment. package Low-level conda package utility. (EXPERIMENTAL) remove Remove a list of packages from a specified conda environment. uninstall Alias for conda remove. run Run an executable in a conda environment. [Experimental] search Search for packages and display associated information. The input is a MatchSpec, a query language for conda packages. See examples below. update Updates conda packages to the latest compatible version. upgrade Alias for conda update. optional arguments: -h, --help Show this help message and exit. -V, --version Show the conda version number and exit. conda commands available from other packages: env  If you see this output, Anaconda has been successfully installed.  You should also have two new applications installed on your computer: Anaconda Navigator and Anaconda Prompt.   conda is shorthand for Anaconda, and its the command we will use to call Anaconda from your computer\u0026rsquo;s terminal to install and manage all of your Python packages.  Check for Python:  In your terminal window, type python as below:  $ \u0026gt; python  Your terminal should now launch a Python prompt, as indicated by the following output:  Python 3.8.10 (default, May 19 2021, 13:12:57) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32 Type \u0026quot;help\u0026quot;, \u0026quot;copyright\u0026quot;, \u0026quot;credits\u0026quot; or \u0026quot;license\u0026quot; for more information. \u0026gt;\u0026gt;\u0026gt;  If you see this output, Python has been successfully installed.  Now, let\u0026rsquo;s learn about creating and managing environments with conda.\nWhat are environments? In Python, a programming environment (also known as a \u0026ldquo;virtual environment\u0026rdquo; or just an \u0026ldquo;environment\u0026rdquo;) is a self-contained ecosystem containing:\n Your Python interpreter. The Python libraries you have installed with conda or pip. The collection of relevant scripts related to your libraries and Python interpreter.  Python environments are isolated from one another, meaning the packages and scripts installed in one environment do not interfere or interact with the packages and scripts in another environment.\nWhy use environments? You should always use a fresh and unique environment for every project you work on. There are four main reasons for this:\n Only installing the libraries necessary for a project to avoid dependency conflicts. Avoiding library version conflicts (e.g., some packages require different versions of the same library). Installing and using different versions Python. Allowing for easy reproducibility of your research and programming environment by providing a list of packages that other scholars can quickly install.  Creating Environments with conda We can create, activate, and manage environments using conda in your computer\u0026rsquo;s terminal.\n Launch your computer\u0026rsquo;s terminal. We\u0026rsquo;re now going to create an environment. I am just going to call it test for now. The syntax for creating a conda environment follows this order:  call conda call the create command provide a name for your environment   Here\u0026rsquo;s the code:  $ \u0026gt; conda create --name test  After running the script, you should have an output that looks like this:  Collecting package metadata (current_repodata.json): done Solving environment: done ## Package Plan ## environment location: **YOUR_PATH_HERE**\\test Proceed ([y]/n)?  Type y and hit enter to continue. You should see the following output:  Preparing transaction: done Verifying transaction: done Executing transaction: done # # To activate this environment, use # # $ conda activate test # # To deactivate an active environment, use # # $ conda deactivate Using the Environment In order to use your conda environment, you need to call the activate command in your terminal. Since we named our environment test, we will tell conda to activate the test environment:\n$ \u0026gt; conda activate test Your terminal should now look something like this (note: this will vary slightly by operating system):\n$ (test) \u0026gt; The parentheses around test tells you your current active conda environment.\nClosing the Environment When you are done using your environment, make sure you deactivate it. This is accomplished easily with the following command:\n$ (test) \u0026gt; conda deactivate Installing Packages There are two primary ways to install packages in Python: (1.) using conda and (2.) using pip. It is recommended to use conda when you can, but in some cases, some libraries are not available in conda\u0026rsquo;s repository. In that case, use pip. It is also easiest to install packages loaded from GitHub with pip as well.\nInstalling Packages with Conda  Note: Before installing packages, make sure you activate your conda environment.\n Usually, you will want to use conda to install Python packages. This can be accomplished easily from the terminal using the install argument:\n$ \u0026gt; conda install PACKAGE_NAME_HERE An important caveat is that conda has differnt channels for installing packages. If you do not specify the channel when you install the package, the installation could take a very long time. Anaconda provides code snippets for all packages in its package repository. Click here to search the repository.\nHere\u0026rsquo;s an example of choosing a channel when installing the numpy package:\n$ \u0026gt; conda install -c conda-forge numpy By calling -c and conda-forge, conda knows to install numpy from the conda-forge channel. Often, specifying -c conda-forge will work, but consult the Anaconda documentation for your particular package.\nInstalling Packages with Pip pip is Python\u0026rsquo;s default package manager. While it comes installed with Python, it is a good idea to install pip with conda within each of your programming environments. This prevents pip from installig packages globally and creating conflicts:\n$ \u0026gt; conda install pip Sometimes, conda will not have a package that you want, and you will need to use pip. In this case, pip is very similar to conda. Let\u0026rsquo;s install the Tabulate package with pip as an example:\n$ \u0026gt; pip install tabulate  Note: Unlike conda, pip does not need to choose a channel to optimize downloads and installs.\n pip is also great when you want to install a pacakge from GitHub, like so:\n$ \u0026gt; pip install git+https://github.com/tabulate/tabulate.git Uninstalling Packages Uninstalling packages follows the same basic syntax:\n$ \u0026gt; conda uninstall PACKAGE_NAME_HERE $ \u0026gt; pip uninstall PACKAGE_NAME_HERE Due to the potential for dependency conflicts, however, you are unlikely to uninstall packages much, if at all.\n Note: Regardless of what compiler or IDE you are using, you must execute your Python scripts from within the environment you have installed your libraries in to use your libraries.\n Using Packages To use the packages you install, you have to import them into your Python program, whether that be a standard Python script, an IDE, or a Jupyter Notebook (this assumes you have activated the appropriate programming environment).\n# Import the default datetime library import datetime # Now you can use the methods from the tabulate package: date = datetime.datetime(2021, 1, 1) # Print year: print(date.year) \u0026gt;\u0026gt;\u0026gt; 2021 Alternatively, you can import submodules of a library to call them directly:\nfrom datetime import datetime date = datetime(2021, 1, 1) print(date.year) \u0026gt;\u0026gt;\u0026gt; 2021 Aliases Many Python packages are imported as aliases, which are usually shorthand representations of the full library name. For example, two extremely popular libraries that you will use on a regular basis, NumPy and Pandas, are aliased as np and pd, respectively:\nimport numpy as np import pandas as pd # Now you can call methods from these libraries with less text: array_one = np.array([1,2,3,4]) array_one.reshape(2,2) \u0026gt;\u0026gt;\u0026gt; array([[1, 2] [3, 4]]) # Load a file: df = pd.read_csv(\u0026#34;your_datac.csv\u0026#34;) df.head() \u0026gt;\u0026gt;\u0026gt; \u0026#34;HEAD_OF_YOUR_DATA_HERE\u0026#34; Summary Figuring out the submodules and aliases of the Python packages you use is something that comes from experience. Consult the API documentation of the packages you use to see specific examples about usage, imports, submodles, and aliases.\nConclusion Congratulations! You have completed this tutorial!\nYou learned:\n How to install Python with the Anaconda distribution. How to create programming environments for your projects. How to use conda and pip to install Python libraries. How to import libraries into your Python programs.  The ins-and-outs of knowing which methods to use from the libraries you install, the specific syntax of libraries and submodules, and the common aliases used by different Python libraries is something that only comes with experience.\nSo get out there, code, and have some fun!\nImportant Resources Here are some useful resources related to this tutorial:\n conda documentation pip documentation conda package repository pip package repository (Python Package Index [PyPI])  ","date":"2021-09-21T00:00:00Z","image":"https://sean-doody.github.io/socilabs/p/install-python/python_hu10dcf594c52feaf835a7b27535ee84db_6082746_120x120_fill_q75_box_smart1.jpg","permalink":"https://sean-doody.github.io/socilabs/p/install-python/","title":"Install Python \u0026 Anaconda"}]